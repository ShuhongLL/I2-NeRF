<!-- PROJECT LOGO -->

<h1 align="center">I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions</h1>
<h3 align="center">üåü NeurIPS 2025 üåü</h3>
<p align="center">
  <a href="https://shuhongll.github.io/">Shuhong Liu</a>, 
  <a href="https://sites.google.com/view/linguedu/home">Lin Gu</a>, 
  <a href="https://cuiziteng.github.io/">Ziteng Cui</a>, 
  <a href="https://xg-chu.site/">Xuangeng Chu</a>, 
  <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada</a>
</p>
<h3 align="center">
  <a href="https://arxiv.org/abs/2510.22161">Arxiv</a> |
  <a href="https://neurips.cc/virtual/2025/poster/118758">Paper</a> |
  <a href="https://shuhongll.github.io/I2_NeRF/">Website</a>
</h3>

<p align="center">
  <img src="assets/uw_cover.gif" width="100%" alt="I2-NeRF demo">
</p>

## ‚òÄÔ∏è Installation
```
git clone git@github.com:ShuhongLL/I2-NeRF.git
```
Our NeRF framework is built on a [PyTorch implementation of ZipNeRF](https://github.com/SuLvXiangXin/zipnerf-pytorch) and uses the same environment configuration. For more setup options (nvdiffrast, DPCPP, etc.), please refer to its installation instructions.
```
conda create --name zipnerf python=3.9
conda activate zipnerf
pip install -r requirements.txt
pip install ./extensions/cuda
```

## üì¶ Dataset
We evaluate low-light scenes using the [LOM dataset](https://github.com/cuiziteng/Aleth-NeRF) ([links](https://drive.google.com/file/d/1orgKEGApjwCm6G8xaupwHKxMbT2s9IAG/view)). For underwater scenes evaluation, we use the [SeaThru-NeRF dataset](https://github.com/deborahLevy130/seathru_NeRF) ([links](https://drive.usercontent.google.com/download?id=1RzojBFvBWjUUhuJb95xJPSNP3nJwZWaT&export=download&authuser=0)).

We provide direct downloads of the rendered test views produced by our model on the LOM and SeaThru-NeRF datasets ([links](https://drive.google.com/drive/folders/1VtbdbytdmS2Xv9OxrsCL9RPtoOUmIzoG?usp=sharing)).

For customized data, we support [Colmap](https://github.com/colmap/colmap), [LLFF](mgs2poses.py), and [Blender](https://github.com/NVlabs/instant-ngp/blob/master/scripts/colmap2nerf.py) format. Please follow the corresponding data preparation instructions for each format.

## ‚öôÔ∏è Configuration

We provide ready-to-run configuration files at `./configs/${dataset}/${scene}.gin` for the LOM and SeaThru-NeRF datasets. The following are several key configuration options:

`Config.enable_absorb=True` enables absorption media branch.

`Config.enable_scatter=True` enables scattering media branch.

`Config.enable_spatial_media=True` enables spatially varying media density. When set to `False`, the model uses per-ray homogeneous media density, which is commonly applied in scattering conditions.

`Config.enable_bcp=True` enables the bright channel prior, which estimates per-pixel illuminance under low-light conditions. When set to `True`, our dataloader automatically computes the pixel-level bcp value.

`Config.enable_depth_prior=True` enables pesudo depth label generated by [DepthAnythingV2](https://github.com/DepthAnything/Depth-Anything-V2) model. We provide the following script to produce a `depth` folder in each scene before training, where `-s` points to the dataset root directory and `-n` specifies the image folder name (e.g., `"images"` in the SeaThru-NeRF dataset or `"low"` in the LOM dataset).
```
python pred_depth.py -s /path/to/data/root/ -n images
```

`Config.luminance_mean=0.5` sets the target luminance level for restored well-lit scenes from low-light inputs.

`Config.contrast_factor=5` sets the contrast enhancement level for restored well-lit scenes from low-light inputs.

We provide a script to automatically compute `luminance_mean` and `contrast_factor` from two arbitrary (paired or unpaired) low-light and reference images:
```python
python pred_llhyp.py -s /path/to/lowlight -t /path/to/ref
```


## üöÄ Training

To run the training on LOM dataset:
```bash
python train.py \
  --gin_configs="configs/LOM/${SCENE}.gin" \
  --gin_bindings="Config.data_dir = '${DATA_DIR}'" \
  --gin_bindings="Config.exp_name = '${EXPERIMENT}'"
```
or simply:
```bash
./scripts/train_lom.sh
```

To run the training on SeaThru-NeRF dataset:
```bash
python train.py \
  --gin_configs=configs/SeaThru/llff_uw.gin \
  --gin_bindings="Config.data_dir = '${DATA_DIR}'" \
  --gin_bindings="Config.exp_name = '${EXPERIMENT}'"
```
or simply:
```bash
./scripts/train_uw.sh
```
The training process typically takes around 40~60 mins, and uses ~20GB memory on a single GPU. To speed up training, you can also run `accelerate launch train.py` to enable multi-GPU training. If you encounter GPU OOM, it is recommended to reduce the training `batch_size` (default 2**14) by specifiying, e.g. `Config.batch_size = 1024`, in the configuration file.

## üìê Evaluation

The evaluation script renders the test views and computes the photometric metrics.

Evaluation on LOM dataset:
```bash
./scripts/eval_lom.sh
```
Evaluation on SeaThru-NeRF dataset:
```bash
./scripts/eval_uw.sh
```

## üé® Render

To individually render the test views, you can run
```bash
python render.py --gin_configs=/path/to/exp/config
```
where `gin_configs` specifies the path to `config.gin` in the generated target experiment directory.


## üìö Acknowledgements
This work builds upon the following repositories:
[multinerf](https://github.com/google-research/multinerf), [zipnerf-pytorch](https://github.com/SuLvXiangXin/zipnerf-pytorch), [Aleth-NeRF](https://github.com/cuiziteng/Aleth-NeRF), [SeaThru-NeRF](https://github.com/deborahLevy130/seathru_NeRF). We thank the authors for making their code publicly available.


## üìù Bibtex
If you find our work useful, please consider citing our paper as:
```bibtex
@inproceedings{liu2025i2nerf,
  title     = {I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions},
  author    = {Liu, Shuhong and Gu, Lin and Cui, Ziteng and Chu, Xuangeng and Harada, Tatsuya},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2025},
}

